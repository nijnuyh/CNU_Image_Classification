{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfTcLf-rjaWd"
      },
      "source": [
        "#### **Attention is All You Need (NIPS 2017)** 실습\n",
        "* <b>(뉴스 데이터셋)</b> 한국어 문장을 영어 문장으로 번역합니다.\n",
        "* 본 코드는 기본적으로 **Transformer** 논문의 내용을 최대한 따릅니다.\n",
        "    * 본 논문은 **딥러닝 기반의 자연어 처리** 기법의 기본적인 구성을 이해하고 공부하는 데에 도움을 줍니다.\n",
        "    * 2020년 기준 가장 뛰어난 번역 모델들은 본 논문에서 제안한 **Transformer 기반의 아키텍처**를 따르고 있습니다.\n",
        "* 코드 실행 전에 **[런타임]** → **[런타임 유형 변경]** → 유형을 **GPU**로 설정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXXXOxNzeLpj"
      },
      "source": [
        "#### <b>한글 출력을 위한 폰트 설치</b>\n",
        "\n",
        "* 설치 이후에 수동으로 <b>[런타임]</b> - <b>[런타임 다시 시작]</b> 버튼을 눌러 재시작합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65nhszH6eNPt",
        "outputId": "e093da78-6b56-4cd4-a4c2-88c362792f0a"
      },
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 9,604 kB of archives.\n",
            "After this operation, 29.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-nanum all 20170925-1 [9,604 kB]\n",
            "Fetched 9,604 kB in 2s (5,652 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 123941 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20170925-1_all.deb ...\n",
            "Unpacking fonts-nanum (20170925-1) ...\n",
            "Setting up fonts-nanum (20170925-1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epJDFH3k03M6"
      },
      "source": [
        "#### <b>BLEU Score 계산을 위한 라이브러리 업데이트</b>\n",
        "\n",
        "* <b>[Restart Runtime]</b> 버튼을 눌러 런타임을 재시작할 필요가 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrr-S31b031u",
        "outputId": "09704bcc-9bfd-4f1f-ade0-40f8f99eba8b"
      },
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.64.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 13.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (4.1.1)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "Successfully installed sentencepiece-0.1.97 torchtext-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OdH8Empjyxa"
      },
      "source": [
        "#### <b>한글 토큰화 라이브러리 설치하기</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2O0iSKl2mT_",
        "outputId": "fbeb37a2-53a4-4b98-f530-f812baf667df"
      },
      "source": [
        "!pip3 install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 69.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5h73Lh9j2GW"
      },
      "source": [
        "#### <b>데이터셋 다운로드</b>\n",
        "\n",
        "* 한영 번역 데이터셋을 다운로드하여 파이썬 객체로 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdPZWIgl2oeF",
        "outputId": "f5cc96ce-7777-4dfd-cdae-7ca33e4ec8a4"
      },
      "source": [
        "# 한영 번역 데이터셋을 포함하는 저장소\n",
        "!git clone https://github.com/ndb796/korean-parallel-corpora"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'korean-parallel-corpora'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Total 131 (delta 0), reused 0 (delta 0), pack-reused 131\u001b[K\n",
            "Receiving objects: 100% (131/131), 17.67 MiB | 16.21 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2iBk4-k2qR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "438dad29-c426-491e-ed67-d86ceb2c5410"
      },
      "source": [
        "# 데이터셋이 저장될 폴더 생성\n",
        "!mkdir -p ./dataset\n",
        "\n",
        "# 압축 해제\n",
        "!tar -xvf ./korean-parallel-corpora/korean-english-news-v1/korean-english-park.train.tar.gz -C ./dataset\n",
        "!tar -xvf ./korean-parallel-corpora/korean-english-news-v1/korean-english-park.test.tar.gz -C ./dataset\n",
        "!tar -xvf ./korean-parallel-corpora/korean-english-news-v1/korean-english-park.dev.tar.gz -C ./dataset\n",
        "\n",
        "# 학습(training) 데이터셋 이름 변경\n",
        "!mv ./dataset/korean-english-park.train.en ./dataset/train.en\n",
        "!mv ./dataset/korean-english-park.train.ko ./dataset/train.ko\n",
        "\n",
        "# 평가(validation) 데이터셋 이름 변경\n",
        "!mv ./dataset/korean-english-park.dev.en ./dataset/dev.en\n",
        "!mv ./dataset/korean-english-park.dev.ko ./dataset/dev.ko\n",
        "\n",
        "# 테스트(test) 데이터셋 이름 변경\n",
        "!mv ./dataset/korean-english-park.test.en ./dataset/test.en\n",
        "!mv ./dataset/korean-english-park.test.ko ./dataset/test.ko"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "korean-english-park.train.en\n",
            "korean-english-park.train.ko\n",
            "korean-english-park.test.en\n",
            "korean-english-park.test.ko\n",
            "korean-english-park.dev.en\n",
            "korean-english-park.dev.ko\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_JbLlFqOpAo"
      },
      "source": [
        "#### <b>데이터셋 읽어 확인하기</b>\n",
        "\n",
        "* 학습, 평가, 테스트 데이터셋을 각각 읽어 문장 데이터를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcTMM_Em2rUJ"
      },
      "source": [
        "korean_lines_train = open(\"./dataset/train.ko\", 'r', encoding='utf-8').readlines()\n",
        "english_lines_train = open(\"./dataset/train.en\", 'r', encoding='utf-8').readlines()\n",
        "\n",
        "korean_lines_val = open(\"./dataset/dev.ko\", 'r', encoding='utf-8').readlines()\n",
        "english_lines_val = open(\"./dataset/dev.en\", 'r', encoding='utf-8').readlines()\n",
        "\n",
        "korean_lines_test = open(\"./dataset/test.ko\", 'r', encoding='utf-8').readlines()\n",
        "english_lines_test = open(\"./dataset/test.en\", 'r', encoding='utf-8').readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np7PF8SFkBNv",
        "outputId": "18ccd90f-d68e-43e2-e641-240d22c55db1"
      },
      "source": [
        "print(f\"한글 문장 학습 데이터 개수: {len(korean_lines_train)}개\")\n",
        "print(f\"영어 문장 학습 데이터 개수: {len(english_lines_train)}개\")\n",
        "\n",
        "print(f\"한글 문장 평가 데이터 개수: {len(korean_lines_val)}개\")\n",
        "print(f\"영어 문장 평가 데이터 개수: {len(english_lines_val)}개\")\n",
        "\n",
        "print(f\"한글 문장 테스트 데이터 개수: {len(korean_lines_test)}개\")\n",
        "print(f\"영어 문장 테스트 데이터 개수: {len(english_lines_test)}개\")\n",
        "\n",
        "index = 777\n",
        "print(f\"{index + 1}번째 학습용 한글 문장:\", korean_lines_train[index], end='')\n",
        "print(f\"{index + 1}번째 학습용 영어 문장:\", english_lines_train[index], end='')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한글 문장 학습 데이터 개수: 94123개\n",
            "영어 문장 학습 데이터 개수: 94123개\n",
            "한글 문장 평가 데이터 개수: 1000개\n",
            "영어 문장 평가 데이터 개수: 1000개\n",
            "한글 문장 테스트 데이터 개수: 2000개\n",
            "영어 문장 테스트 데이터 개수: 2000개\n",
            "778번째 학습용 한글 문장: 지금 21살인 유는 학교에 가기 전 서너시간 동안 컴퓨터 통신에 끼어들기 위해 새벽 5시에 침대에서 일어나 나온다.\n",
            "778번째 학습용 영어 문장: Now Yu, 21, drags herself out of bed at 5 a.m. to squeeze in a few hours online before school.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcLKB3BRkDKW"
      },
      "source": [
        "#### <b>단어 사전 만들기 </b>\n",
        "\n",
        "* 단어 사전 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWpaBry12tjd"
      },
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self):\n",
        "        self.UNK = '<unk>'\n",
        "        self.PAD = '<pad>'\n",
        "        self.SOS = '<sos>'\n",
        "        self.EOS = '<eos>'\n",
        "\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.word2count = {}\n",
        "\n",
        "    # 하나의 문장(sentence)에 포함된 모든 토큰을 추가하는 함수\n",
        "    def add_tokens(self, tokens):\n",
        "        for word in tokens:\n",
        "            if word in self.word2count:\n",
        "                self.word2count[word] += 1\n",
        "            else:\n",
        "                self.word2count[word] = 1\n",
        "\n",
        "    def preprocess(self, min_count):\n",
        "        # 사용하지 않을 단어 집합\n",
        "        trim_words = set()\n",
        "        for word, count in self.word2count.items():\n",
        "            if count < min_count:\n",
        "                trim_words.add(word)\n",
        "\n",
        "        # 실제로 사용할 단어만 남기기\n",
        "        words = set(self.word2count.keys()) - trim_words\n",
        "        words = [self.UNK, self.PAD, self.SOS, self.EOS] + list(words)\n",
        "\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        for i, word in enumerate(words):\n",
        "            self.word2idx[word] = i\n",
        "            self.idx2word[i] = word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSB5vFULkecQ"
      },
      "source": [
        "#### <b>문장 토큰화</b>\n",
        "\n",
        "* 먼저 한글 문장 및 영어 문장 데이터셋에 대하여 토큰화를 수행합니다.\n",
        "* 토큰화를 위해 특수문자 제거 함수를 정의하고 객체를 초기화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDH1f2gqlxO-"
      },
      "source": [
        "import re\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "def clean_string(string):\n",
        "    string = string.strip() # 앞뒤로 존재하는 공백 제거\n",
        "    string = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', string) # 특수문자 제거\n",
        "    return string.strip().lower() # 소문자로 변환하여 반환\n",
        "\n",
        "okt = Okt() # 한글 형태소 분석기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fP80_AZmSC8"
      },
      "source": [
        "* 학습(training) 데이터셋을 토큰화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyKq4u8pmRlM",
        "outputId": "0cf9f475-1028-4796-c8e6-b023bc464bdb"
      },
      "source": [
        "tokenized_korean_lines_train = []\n",
        "tokenized_english_lines_train = []\n",
        "\n",
        "min_length = 4 # 단어의 개수가 4개 이상인 학습 문장 쌍만 사용\n",
        "max_length = 50 # 단어의 개수가 50개 이하인 학습 문장 쌍만 사용\n",
        "\n",
        "for i in range(len(korean_lines_train)):\n",
        "    korean = korean_lines_train[i]\n",
        "    korean = clean_string(korean)\n",
        "    korean_tokens = [line[0] for line in okt.pos(korean, norm=True)] # 한글 형태소 분석 결과 추출\n",
        "\n",
        "    english = english_lines_train[i]\n",
        "    english = clean_string(english)\n",
        "    english_tokens = english.split(' ')\n",
        "\n",
        "    if len(korean_tokens) < min_length or len(korean_tokens) > max_length:\n",
        "        continue\n",
        "    if len(english_tokens) < min_length or len(english_tokens) > max_length:\n",
        "        continue\n",
        "\n",
        "    tokenized_korean_lines_train.append(korean_tokens)\n",
        "    tokenized_english_lines_train.append(english_tokens)\n",
        "\n",
        "    if (i + 1) % 4000 == 0:\n",
        "        print(f\"학습 데이터셋 토큰화: {i + 1}/{len(korean_lines_train)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 데이터셋 토큰화: 4000/94123\n",
            "학습 데이터셋 토큰화: 8000/94123\n",
            "학습 데이터셋 토큰화: 12000/94123\n",
            "학습 데이터셋 토큰화: 16000/94123\n",
            "학습 데이터셋 토큰화: 20000/94123\n",
            "학습 데이터셋 토큰화: 24000/94123\n",
            "학습 데이터셋 토큰화: 28000/94123\n",
            "학습 데이터셋 토큰화: 32000/94123\n",
            "학습 데이터셋 토큰화: 36000/94123\n",
            "학습 데이터셋 토큰화: 40000/94123\n",
            "학습 데이터셋 토큰화: 44000/94123\n",
            "학습 데이터셋 토큰화: 48000/94123\n",
            "학습 데이터셋 토큰화: 52000/94123\n",
            "학습 데이터셋 토큰화: 56000/94123\n",
            "학습 데이터셋 토큰화: 60000/94123\n",
            "학습 데이터셋 토큰화: 64000/94123\n",
            "학습 데이터셋 토큰화: 68000/94123\n",
            "학습 데이터셋 토큰화: 72000/94123\n",
            "학습 데이터셋 토큰화: 76000/94123\n",
            "학습 데이터셋 토큰화: 80000/94123\n",
            "학습 데이터셋 토큰화: 84000/94123\n",
            "학습 데이터셋 토큰화: 88000/94123\n",
            "학습 데이터셋 토큰화: 92000/94123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DerYg_C1nOk7"
      },
      "source": [
        "* 평가(validation) 데이터셋을 토큰화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKrnkrbTnPoc",
        "outputId": "f3ab59fc-b243-422f-8f32-7682af2f93c4"
      },
      "source": [
        "tokenized_korean_lines_val = []\n",
        "tokenized_english_lines_val = []\n",
        "\n",
        "for i in range(len(korean_lines_val)):\n",
        "    korean = korean_lines_val[i]\n",
        "    korean = clean_string(korean)\n",
        "    korean_tokens = [line[0] for line in okt.pos(korean, norm=True)] # 한글 형태소 분석 결과 추출\n",
        "\n",
        "    english = english_lines_val[i]\n",
        "    english = clean_string(english)\n",
        "    english_tokens = english.split(' ')\n",
        "\n",
        "    tokenized_korean_lines_val.append(korean_tokens)\n",
        "    tokenized_english_lines_val.append(english_tokens)\n",
        "\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"평가 데이터셋 토큰화: {i + 1}/{len(korean_lines_val)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "평가 데이터셋 토큰화: 1000/1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_sP1Wn3pvQU"
      },
      "source": [
        "* 테스트(test) 데이터셋을 토큰화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxY4lieapyQj",
        "outputId": "2385d188-5a84-4fd6-f1dc-b3d4552a0098"
      },
      "source": [
        "tokenized_korean_lines_test = []\n",
        "tokenized_english_lines_test = []\n",
        "\n",
        "for i in range(len(korean_lines_test)):\n",
        "    korean = korean_lines_test[i]\n",
        "    korean = clean_string(korean)\n",
        "    korean_tokens = [line[0] for line in okt.pos(korean, norm=True)] # 한글 형태소 분석 결과 추출\n",
        "\n",
        "    english = english_lines_test[i]\n",
        "    english = clean_string(english)\n",
        "    english_tokens = english.split(' ')\n",
        "\n",
        "    tokenized_korean_lines_test.append(korean_tokens)\n",
        "    tokenized_english_lines_test.append(english_tokens)\n",
        "\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"테스트 데이터셋 토큰화: {i + 1}/{len(korean_lines_test)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 데이터셋 토큰화: 1000/2000\n",
            "테스트 데이터셋 토큰화: 2000/2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxvJN5THq2hx"
      },
      "source": [
        "#### <b>단어 사전 만들기</b>\n",
        "\n",
        "* 최소 2번 이상 등장한 단어만 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNVKZ7NbrP2l",
        "outputId": "73c1a709-2332-4259-abbf-88f062df0696"
      },
      "source": [
        "korean_voca = Vocabulary()\n",
        "english_voca = Vocabulary()\n",
        "\n",
        "for i in range(len(tokenized_korean_lines_train)):\n",
        "    korean_tokens = tokenized_korean_lines_train[i]\n",
        "    english_tokens = tokenized_english_lines_train[i]\n",
        "\n",
        "    korean_voca.add_tokens(korean_tokens)\n",
        "    english_voca.add_tokens(english_tokens)\n",
        "\n",
        "korean_voca.preprocess(min_count=2)\n",
        "english_voca.preprocess(min_count=2)\n",
        "\n",
        "print(\"전체 한국어 단어 수:\", len(korean_voca.word2count))\n",
        "print(\"전체 영어 단어 수:\", len(english_voca.word2count))\n",
        "print(\"사용할 한국어 토큰 수:\", len(korean_voca.word2idx))\n",
        "print(\"사용할 영어 토큰 수:\", len(english_voca.word2idx))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 한국어 단어 수: 67029\n",
            "전체 영어 단어 수: 58833\n",
            "사용할 한국어 토큰 수: 40612\n",
            "사용할 영어 토큰 수: 35745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wyKQjwAusyw",
        "outputId": "f77bde2e-59be-4677-96f8-e65dadc44876"
      },
      "source": [
        "print(korean_voca.word2idx['<pad>']) # 패딩(padding): 1\n",
        "print(korean_voca.word2idx['<sos>']) # <sos>: 2\n",
        "print(korean_voca.word2idx['<eos>']) # <eos>: 3\n",
        "print(korean_voca.word2idx['컴퓨터'])\n",
        "print(korean_voca.word2idx['사랑'])\n",
        "print(korean_voca.word2idx['기적'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "40328\n",
            "26907\n",
            "1356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQjD1bjVuvHt",
        "outputId": "720f070d-a532-4505-9aa1-f454db3ff6ed"
      },
      "source": [
        "print(english_voca.word2idx['<pad>']) # 패딩(padding): 1\n",
        "print(english_voca.word2idx['<sos>']) # <sos>: 2\n",
        "print(english_voca.word2idx['<eos>']) # <eos>: 3\n",
        "print(english_voca.word2idx['computer'])\n",
        "print(english_voca.word2idx['love'])\n",
        "print(english_voca.word2idx['miracle'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "33940\n",
            "6181\n",
            "2840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw7jo7dmsHRW"
      },
      "source": [
        "* Unknown Token이 1개 이상 포함된 문장은 데이터셋에서 제외하여 다시 학습 데이터셋을 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M717ROWBsFmM"
      },
      "source": [
        "unknown_threshold = 1\n",
        "\n",
        "preprocessed_korean_lines_train = []\n",
        "preprocessed_english_lines_train = []\n",
        "\n",
        "for i in range(len(tokenized_korean_lines_train)):\n",
        "    korean_tokens = tokenized_korean_lines_train[i]\n",
        "    english_tokens = tokenized_english_lines_train[i]\n",
        "\n",
        "    is_used = True # 현재의 문장 쌍을 사용할지의 여부\n",
        "    for token in korean_tokens:\n",
        "        cnt = 0\n",
        "        if token not in korean_voca.word2idx:\n",
        "            cnt += 1\n",
        "        if cnt >= unknown_threshold:\n",
        "            is_used = False\n",
        "    for token in english_tokens:\n",
        "        cnt = 0\n",
        "        if token not in english_voca.word2idx:\n",
        "            cnt += 1\n",
        "        if cnt >= unknown_threshold:\n",
        "            is_used = False\n",
        "\n",
        "    if not is_used:\n",
        "        continue\n",
        "\n",
        "    preprocessed_korean_lines_train.append(korean_tokens)\n",
        "    preprocessed_english_lines_train.append(english_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg6lc1V4uDQ6",
        "outputId": "365210e5-eb92-4709-e41b-af761f609e2f"
      },
      "source": [
        "print(\"사용할 한국어 학습 문장 수:\", len(preprocessed_korean_lines_train))\n",
        "print(\"사용할 영어 학습 문장 수:\", len(preprocessed_english_lines_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용할 한국어 학습 문장 수: 61029\n",
            "사용할 영어 학습 문장 수: 61029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH1SIu_i5mch",
        "outputId": "14c5fc4f-fff5-4b02-b335-e8048a5cd7f0"
      },
      "source": [
        "print(preprocessed_korean_lines_train[7777])\n",
        "print(preprocessed_english_lines_train[7777])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cnn', '의', '여론조사', '국장', '인', '키팅', '홀랜드', '는', '“', '이라크전', '발발', '직후', '부시', '의', '지지도', '는', '71', '였다', '”', '며', '“', '지지율', '40', '추락', '은', '베트남전', '당시', '린', '든', '존슨', '대통령', '과', '유사하다', '”', '고', '지적', '했다']\n",
            "['bushs', 'approval', 'rating', 'five', 'years', 'ago', 'at', 'the', 'start', 'of', 'the', 'iraq', 'war', 'was', '71', 'percent', 'and', 'that', '40point', 'drop', 'is', 'almost', 'identical', 'to', 'the', 'drop', 'president', 'lyndon', 'johnson', 'faced', 'during', 'the', 'vietnam', 'war', 'said', 'cnn', 'polling', 'director', 'keating', 'holland']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eItQjid_uzsB"
      },
      "source": [
        "#### <b>커스텀 데이터셋 클래스 작성하기</b>\n",
        "\n",
        "* 소스 문장(한국어)과 타겟 문장(영어)를 한 쌍으로 반환하는 데이터셋 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C7JV5PE5rmM"
      },
      "source": [
        "import copy\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, korean_lines, english_lines, max_seq_len):\n",
        "        self.korean_lines = korean_lines\n",
        "        self.english_lines = english_lines\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoder_input = self.get_encoder_input(self.korean_lines[index])\n",
        "        decoder_input = self.get_decoder_input(self.english_lines[index])\n",
        "\n",
        "        return encoder_input, decoder_input\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.korean_lines)\n",
        "\n",
        "    # 한글 문장 벡터화\n",
        "    def get_encoder_input(self, tokens):\n",
        "        tokens = copy.deepcopy(tokens)\n",
        "        tokens.insert(0, korean_voca.SOS)\n",
        "        tokens.append(korean_voca.EOS)\n",
        "        tokens = self.padding(tokens, korean_voca) # 문장 뒤쪽에 패딩 붙이기\n",
        "        index_list = self.word2idx(tokens, korean_voca)\n",
        "\n",
        "        return torch.tensor(index_list).to(device)\n",
        "\n",
        "    # 영어 문장 벡터화\n",
        "    def get_decoder_input(self, tokens):\n",
        "        tokens = copy.deepcopy(tokens)\n",
        "        tokens.insert(0, english_voca.SOS)\n",
        "        tokens.append(english_voca.EOS)\n",
        "        tokens = self.padding(tokens, english_voca) # 문장 뒤쪽에 패딩 붙이기\n",
        "        index_list = self.word2idx(tokens, english_voca)\n",
        "\n",
        "        return torch.tensor(index_list).to(device)\n",
        "\n",
        "    # max_seq_len보다 길이가 짧은 문장에 대해 <pad> 토큰 채우기\n",
        "    def padding(self, tokens, voca):\n",
        "        if len(tokens) < self.max_seq_len:\n",
        "            tokens += [voca.PAD] * (self.max_seq_len - len(tokens))\n",
        "        else:\n",
        "            tokens = tokens[:self.max_seq_len]\n",
        "        return tokens\n",
        "\n",
        "    def word2idx(self, tokens, voca):\n",
        "        idx_list = []\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                idx_list.append(voca.word2idx[token])\n",
        "            except KeyError:\n",
        "                idx_list.append(voca.word2idx[voca.UNK])\n",
        "        return idx_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EObNRkSSvPfk"
      },
      "source": [
        "* 학습/평가/테스트 데이터셋 객체를 초기화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBsC-CRc5_F7"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_dataset = CustomDataset(preprocessed_korean_lines_train, preprocessed_english_lines_train, max_seq_len=80)\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=128, num_workers=0)\n",
        "\n",
        "val_dataset = CustomDataset(tokenized_korean_lines_val, tokenized_english_lines_val, max_seq_len=80)\n",
        "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=128, num_workers=0)\n",
        "\n",
        "test_dataset = CustomDataset(tokenized_korean_lines_test, tokenized_english_lines_test, max_seq_len=80)\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=128, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlKQSsxK6BdK",
        "outputId": "1e0d484f-5c56-4f3d-ab10-cf46c9d2d727"
      },
      "source": [
        "# 하나의 배치에 포함되어 있는 문장을 출력합니다.\n",
        "for i, batch in enumerate(train_loader):\n",
        "    src = batch[0]\n",
        "    trg = batch[1]\n",
        "\n",
        "    print(f\"첫 번째 배치 크기: {src.shape}\")\n",
        "\n",
        "    # 현재 배치에 있는 하나의 문장에 포함된 정보 출력\n",
        "    for i in range(src.shape[1]):\n",
        "        print(f\"인덱스 {i}: {src[0][i].item()}\") # 여기에서는 [Seq_num, Seq_len]\n",
        "\n",
        "    # 첫 번째 배치만 확인\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 배치 크기: torch.Size([128, 80])\n",
            "인덱스 0: 2\n",
            "인덱스 1: 7543\n",
            "인덱스 2: 12855\n",
            "인덱스 3: 34363\n",
            "인덱스 4: 14467\n",
            "인덱스 5: 3300\n",
            "인덱스 6: 11547\n",
            "인덱스 7: 28822\n",
            "인덱스 8: 24114\n",
            "인덱스 9: 27188\n",
            "인덱스 10: 31254\n",
            "인덱스 11: 8756\n",
            "인덱스 12: 11547\n",
            "인덱스 13: 18445\n",
            "인덱스 14: 12999\n",
            "인덱스 15: 4552\n",
            "인덱스 16: 38774\n",
            "인덱스 17: 28980\n",
            "인덱스 18: 23928\n",
            "인덱스 19: 32235\n",
            "인덱스 20: 35501\n",
            "인덱스 21: 22592\n",
            "인덱스 22: 12999\n",
            "인덱스 23: 21567\n",
            "인덱스 24: 18882\n",
            "인덱스 25: 37253\n",
            "인덱스 26: 26000\n",
            "인덱스 27: 29652\n",
            "인덱스 28: 12999\n",
            "인덱스 29: 24618\n",
            "인덱스 30: 27294\n",
            "인덱스 31: 699\n",
            "인덱스 32: 28491\n",
            "인덱스 33: 14485\n",
            "인덱스 34: 5324\n",
            "인덱스 35: 17360\n",
            "인덱스 36: 10907\n",
            "인덱스 37: 15980\n",
            "인덱스 38: 15516\n",
            "인덱스 39: 577\n",
            "인덱스 40: 2204\n",
            "인덱스 41: 11187\n",
            "인덱스 42: 3\n",
            "인덱스 43: 1\n",
            "인덱스 44: 1\n",
            "인덱스 45: 1\n",
            "인덱스 46: 1\n",
            "인덱스 47: 1\n",
            "인덱스 48: 1\n",
            "인덱스 49: 1\n",
            "인덱스 50: 1\n",
            "인덱스 51: 1\n",
            "인덱스 52: 1\n",
            "인덱스 53: 1\n",
            "인덱스 54: 1\n",
            "인덱스 55: 1\n",
            "인덱스 56: 1\n",
            "인덱스 57: 1\n",
            "인덱스 58: 1\n",
            "인덱스 59: 1\n",
            "인덱스 60: 1\n",
            "인덱스 61: 1\n",
            "인덱스 62: 1\n",
            "인덱스 63: 1\n",
            "인덱스 64: 1\n",
            "인덱스 65: 1\n",
            "인덱스 66: 1\n",
            "인덱스 67: 1\n",
            "인덱스 68: 1\n",
            "인덱스 69: 1\n",
            "인덱스 70: 1\n",
            "인덱스 71: 1\n",
            "인덱스 72: 1\n",
            "인덱스 73: 1\n",
            "인덱스 74: 1\n",
            "인덱스 75: 1\n",
            "인덱스 76: 1\n",
            "인덱스 77: 1\n",
            "인덱스 78: 1\n",
            "인덱스 79: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bieO3YwVvmLY"
      },
      "source": [
        "#### **Multi Head Attention 아키텍처**\n",
        "\n",
        "* 어텐션(attention)은 <b>세 가지 요소</b>를 입력으로 받습니다.\n",
        "    * <b>쿼리(queries)</b>\n",
        "    * <b>키(keys)</b>\n",
        "    * <b>값(values)</b>\n",
        "    * 현재 구현에서는 Query, Key, Value의 차원이 모두 같습니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRX0AoF1voKW"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n",
        "        super().__init__()\n",
        "\n",
        "        assert hidden_dim % n_heads == 0\n",
        "\n",
        "        self.hidden_dim = hidden_dim # 임베딩 차원\n",
        "        self.n_heads = n_heads # 헤드(head)의 개수: 서로 다른 어텐션(attention) 컨셉의 수\n",
        "        self.head_dim = hidden_dim // n_heads # 각 헤드(head)에서의 임베딩 차원\n",
        "\n",
        "        self.fc_q = nn.Linear(hidden_dim, hidden_dim) # Query 값에 적용될 FC 레이어\n",
        "        self.fc_k = nn.Linear(hidden_dim, hidden_dim) # Key 값에 적용될 FC 레이어\n",
        "        self.fc_v = nn.Linear(hidden_dim, hidden_dim) # Value 값에 적용될 FC 레이어\n",
        "\n",
        "        self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # query: [batch_size, query_len, hidden_dim]\n",
        "        # key: [batch_size, key_len, hidden_dim]\n",
        "        # value: [batch_size, value_len, hidden_dim]\n",
        " \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "\n",
        "        # Q: [batch_size, query_len, hidden_dim]\n",
        "        # K: [batch_size, key_len, hidden_dim]\n",
        "        # V: [batch_size, value_len, hidden_dim]\n",
        "\n",
        "        # hidden_dim → n_heads X head_dim 형태로 변형\n",
        "        # n_heads(h)개의 서로 다른 어텐션(attention) 컨셉을 학습하도록 유도\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Q: [batch_size, n_heads, query_len, head_dim]\n",
        "        # K: [batch_size, n_heads, key_len, head_dim]\n",
        "        # V: [batch_size, n_heads, value_len, head_dim]\n",
        "\n",
        "        # Attention Energy 계산\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "        # energy: [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        # 마스크(mask)를 사용하는 경우\n",
        "        if mask is not None:\n",
        "            # 마스크(mask) 값이 0인 부분을 -1e10으로 채우기\n",
        "            energy = energy.masked_fill(mask==0, -1e10)\n",
        "\n",
        "        # 어텐션(attention) 스코어 계산: 각 단어에 대한 확률 값\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "\n",
        "        # attention: [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        # 여기에서 Scaled Dot-Product Attention을 계산\n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "\n",
        "        # x: [batch_size, n_heads, query_len, head_dim]\n",
        "\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "        # x: [batch_size, query_len, n_heads, head_dim]\n",
        "\n",
        "        x = x.view(batch_size, -1, self.hidden_dim)\n",
        "\n",
        "        # x: [batch_size, query_len, hidden_dim]\n",
        "\n",
        "        x = self.fc_o(x)\n",
        "\n",
        "        # x: [batch_size, query_len, hidden_dim]\n",
        "\n",
        "        return x, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzSDUlmOvq6-"
      },
      "source": [
        "#### **Position-wise Feedforward 아키텍처**\n",
        "\n",
        "* 입력과 출력의 차원이 동일합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBfNsiED6LSe"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc_1 = nn.Linear(hidden_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # x: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "\n",
        "        # x: [batch_size, seq_len, pf_dim]\n",
        "\n",
        "        x = self.fc_2(x)\n",
        "\n",
        "        # x: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz6fWXndvvwJ"
      },
      "source": [
        "#### **인코더(Encoder) 레이어 아키텍처**\n",
        "\n",
        "* 하나의 인코더 레이어에 대해 정의합니다.\n",
        "    * 입력과 출력의 차원이 같습니다.\n",
        "    * 이러한 특징을 이용해 트랜스포머의 인코더는 인코더 레이어를 여러 번 중첩해 사용합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율\n",
        "* &lt;pad&gt; 토큰에 대하여 마스크(mask) 값을 0으로 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwhr1-df6MKk"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 하나의 임베딩이 복제되어 Query, Key, Value로 입력되는 방식\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        # self attention\n",
        "        # 필요한 경우 마스크(mask) 행렬을 이용하여 어텐션(attention)할 단어를 조절 가능\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "\n",
        "        # dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        # position-wise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "\n",
        "        # dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        return src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N22uqlEmvx06"
      },
      "source": [
        "#### **인코더(Encoder) 아키텍처**\n",
        "\n",
        "* 전체 인코더 아키텍처를 정의합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **input_dim**: 하나의 단어에 대한 원 핫 인코딩 차원\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_layers**: 내부적으로 사용할 인코더 레이어의 개수\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율\n",
        "    * **max_length**: 문장 내 최대 단어 개수\n",
        "* 원본 논문과는 다르게 <b>위치 임베딩(positional embedding)을 학습</b>하는 형태로 구현합니다.\n",
        "    * BERT와 같은 모던 트랜스포머 아키텍처에서 사용되는 방식입니다.\n",
        "* &lt;pad&gt; 토큰에 대하여 마스크(mask) 값을 0으로 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j2ZRWaf6M14"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        # src: [batch_size, src_len]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "        # pos: [batch_size, src_len]\n",
        "\n",
        "        # 소스 문장의 임베딩과 위치 임베딩을 더한 것을 사용\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        # 모든 인코더 레이어를 차례대로 거치면서 순전파(forward) 수행\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        return src # 마지막 레이어의 출력을 반환"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl-eh0mAvzva"
      },
      "source": [
        "#### **디코더(Decoder) 레이어 아키텍처**\n",
        "\n",
        "* 하나의 디코더 레이어에 대해 정의합니다.\n",
        "    * 입력과 출력의 차원이 같습니다.\n",
        "    * 이러한 특징을 이용해 트랜스포머의 디코더는 디코더 레이어를 여러 번 중첩해 사용합니다.\n",
        "    * 디코더 레이어에서는 두 개의 Multi-Head Attention 레이어가 사용됩니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율\n",
        "* 소스 문장의 &lt;pad&gt; 토큰에 대하여 마스크(mask) 값을 0으로 설정합니다.\n",
        "* 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBalCQMq6Nj4"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 인코더의 출력 값(enc_src)을 어텐션(attention)하는 구조\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\n",
        "        # trg_mask: [batch_size, trg_len]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        # self attention\n",
        "        # 자기 자신에 대하여 어텐션(attention)\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "\n",
        "        # dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "        # encoder attention\n",
        "        # 디코더의 쿼리(Query)를 이용해 인코더를 어텐션(attention)\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "\n",
        "        # dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "        # positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "\n",
        "        # dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        return trg, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kRYUk-1v1jv"
      },
      "source": [
        "#### **디코더(Decoder) 아키텍처**\n",
        "\n",
        "* 전체 디코더 아키텍처를 정의합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **output_dim**: 하나의 단어에 대한 원 핫 인코딩 차원\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_layers**: 내부적으로 사용할 인코더 레이어의 개수\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율\n",
        "    * **max_length**: 문장 내 최대 단어 개수\n",
        "* 원본 논문과는 다르게 <b>위치 임베딩(positional embedding)을 학습</b>하는 형태로 구현합니다.\n",
        "    * BERT와 같은 모던 트랜스포머 아키텍처에서 사용되는 방식입니다.\n",
        "* Seq2Seq과는 마찬가지로 실제로 추론(inference) 시기에서는 디코더를 반복적으로 넣을 필요가 있습니다.\n",
        "    * 학습(training) 시기에서는 한 번에 출력 문장을 구해 학습할 수 있습니다.\n",
        "* 소스 문장의 &lt;pad&gt; 토큰에 대하여 마스크(mask) 값을 0으로 설정합니다.\n",
        "* 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzJ-DCDt6Oev"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "\n",
        "        # trg: [batch_size, trg_len]\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\n",
        "        # trg_mask: [batch_size, trg_len]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "        # pos: [batch_size, trg_len]\n",
        "\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # 소스 마스크와 타겟 마스크 모두 사용\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        output = self.fc_out(trg)\n",
        "\n",
        "        # output: [batch_size, trg_len, output_dim]\n",
        "\n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn-yCx4yv348"
      },
      "source": [
        "#### **트랜스포머(Transformer) 아키텍처**\n",
        "\n",
        "* 최종적인 전체 트랜스포머(Transformer) 모델을 정의합니다.\n",
        "* 입력이 들어왔을 때 앞서 정의한 인코더와 디코더를 거쳐 출력 문장을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ignEkCL6PSr"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    # 소스 문장의 <pad> 토큰에 대하여 마스크(mask) 값을 0으로 설정\n",
        "    def make_src_mask(self, src):\n",
        "\n",
        "        # src: [batch_size, src_len]\n",
        "\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\n",
        "\n",
        "        return src_mask\n",
        "\n",
        "    # 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용\n",
        "    def make_trg_mask(self, trg):\n",
        "\n",
        "        # trg: [batch_size, trg_len]\n",
        "\n",
        "        \"\"\" (마스크 예시)\n",
        "        1 0 0 0 0\n",
        "        1 1 0 0 0\n",
        "        1 1 1 0 0\n",
        "        1 1 1 0 0\n",
        "        1 1 1 0 0\n",
        "        \"\"\"\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
        "\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        \"\"\" (마스크 예시)\n",
        "        1 0 0 0 0\n",
        "        1 1 0 0 0\n",
        "        1 1 1 0 0\n",
        "        1 1 1 1 0\n",
        "        1 1 1 1 1\n",
        "        \"\"\"\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "\n",
        "        # trg_sub_mask: [trg_len, trg_len]\n",
        "\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "\n",
        "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "\n",
        "        # src: [batch_size, src_len]\n",
        "        # trg: [batch_size, trg_len]\n",
        "\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\n",
        "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # output: [batch_size, trg_len, output_dim]\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1TDOflzv5xW"
      },
      "source": [
        "#### **학습(Training)**\n",
        "\n",
        "* 하이퍼 파라미터 설정 및 모델 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ombHNgO56Q7r"
      },
      "source": [
        "INPUT_DIM = len(korean_voca.word2idx)\n",
        "OUTPUT_DIM = len(english_voca.word2idx)\n",
        "HIDDEN_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W-Tr-D06SbJ"
      },
      "source": [
        "SRC_PAD_IDX = korean_voca.word2idx[korean_voca.PAD]\n",
        "TRG_PAD_IDX = english_voca.word2idx[english_voca.PAD]\n",
        "\n",
        "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
        "enc = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
        "dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
        "\n",
        "# Transformer 객체 선언\n",
        "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1DbVBkVwCvA"
      },
      "source": [
        "* **모델 가중치 파라미터 초기화**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XObg1hxB6bOA",
        "outputId": "1cdbe46c-2624-4bef-9eb7-976568bb14a9"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 32,738,721 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWItEfu26chy",
        "outputId": "7f71e953-f0a8-414b-8d21-1960dc789d48"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(40612, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(35745, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=35745, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Je4gMnSwITT"
      },
      "source": [
        "* 학습 및 평가 함수 정의\n",
        "    * 기본적인 Seq2Seq 모델과 거의 유사하게 작성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_hn64HD6dYK"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Adam optimizer로 학습 최적화\n",
        "LEARNING_RATE = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg6MC3dK6gF9"
      },
      "source": [
        "# 모델 학습(train) 함수\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train() # 학습 모드\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # 전체 학습 데이터를 확인하며\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch[0]\n",
        "        trg = batch[1]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
        "        # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "\n",
        "        # output: [배치 크기, trg_len - 1, output_dim]\n",
        "        # trg: [배치 크기, trg_len]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        # 출력 단어의 인덱스 0(<sos>)은 제외\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "        # output: [배치 크기 * trg_len - 1, output_dim]\n",
        "        # trg: [배치 크기 * trg len - 1]\n",
        "\n",
        "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward() # 기울기(gradient) 계산\n",
        "\n",
        "        # 기울기(gradient) clipping 진행\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 전체 손실 값 계산\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEukz7xc6o6F"
      },
      "source": [
        "# 모델 평가(evaluate) 함수\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval() # 평가 모드\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 전체 평가 데이터를 확인하며\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch[0]\n",
        "            trg = batch[1]\n",
        "\n",
        "            # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
        "            # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "\n",
        "            # output: [배치 크기, trg_len - 1, output_dim]\n",
        "            # trg: [배치 크기, trg_len]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            # 출력 단어의 인덱스 0(<sos>)은 제외\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "            # output: [배치 크기 * trg_len - 1, output_dim]\n",
        "            # trg: [배치 크기 * trg len - 1]\n",
        "\n",
        "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            # 전체 손실 값 계산\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4-D3IuNwO9t"
      },
      "source": [
        "* 학습(training) 및 검증(validation) 진행\n",
        "    * **학습 횟수(epoch)**: 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuAulfy_6pow"
      },
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC-dgh1E6qbF",
        "outputId": "ed271f5b-e518-4d22-c155-5cffba075b27"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "N_EPOCHS = 4\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time() # 시작 시간 기록\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    end_time = time.time() # 종료 시간 기록\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'transformer_korean_to_english.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 2m 50s\n",
            "\tTrain Loss: 6.421 | Train PPL: 614.574\n",
            "\tValidation Loss: 5.998 | Validation PPL: 402.657\n",
            "Epoch: 02 | Time: 2m 53s\n",
            "\tTrain Loss: 5.087 | Train PPL: 161.854\n",
            "\tValidation Loss: 5.641 | Validation PPL: 281.864\n",
            "Epoch: 03 | Time: 2m 54s\n",
            "\tTrain Loss: 4.380 | Train PPL: 79.838\n",
            "\tValidation Loss: 5.542 | Validation PPL: 255.206\n",
            "Epoch: 04 | Time: 2m 54s\n",
            "\tTrain Loss: 3.812 | Train PPL: 45.263\n",
            "\tValidation Loss: 5.610 | Validation PPL: 273.265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw1ntWEV6uth"
      },
      "source": [
        "# 번역(translation) 함수\n",
        "def translate_sentence(korean, model, device, max_len=80, logging=True):\n",
        "    model.eval() # 평가 모드\n",
        "\n",
        "    korean = clean_string(korean)\n",
        "    tokens = [line[0] for line in okt.pos(korean, norm=True)] # 한글 형태소 분석 결과 추출\n",
        "\n",
        "    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n",
        "    tokens = [korean_voca.SOS] + tokens + [korean_voca.EOS]\n",
        "    if logging:\n",
        "        print(f\"전체 소스 토큰: {tokens}\")\n",
        "\n",
        "    src_indexes = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            src_indexes.append(korean_voca.word2idx[token])\n",
        "        except KeyError:\n",
        "            src_indexes.append(korean_voca.word2idx[korean_voca.UNK])\n",
        "    if logging:\n",
        "        print(f\"소스 문장 인덱스: {src_indexes}\")\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "    # 소스 문장에 따른 마스크 생성\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    # 인코더(endocer)에 소스 문장을 넣어 문맥 벡터(context vector) 계산\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n",
        "    trg_indexes = [english_voca.word2idx[english_voca.SOS]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        # 출력 문장에 따른 마스크 생성\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # 출력 문장에서 가장 마지막 단어만 사용\n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
        "\n",
        "        # <eos>를 만나는 순간 끝\n",
        "        if pred_token == english_voca.word2idx[english_voca.EOS]:\n",
        "            break\n",
        "\n",
        "    # 각 출력 단어 인덱스를 실제 단어로 변환\n",
        "    trg_tokens = [english_voca.idx2word[i] for i in trg_indexes]\n",
        "\n",
        "    # 첫 번째 <sos>는 제외하고 출력 문장 반환\n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avx76dMr9mXm",
        "outputId": "92540dbb-112f-4516-c559-6a7fd8afe9f3"
      },
      "source": [
        "example_idx = 15\n",
        "\n",
        "src = korean_lines_test[example_idx]\n",
        "trg = english_lines_test[example_idx]\n",
        "\n",
        "print(f'소스 문장: {src}', end='')\n",
        "print(f'타겟 문장: {trg}', end='')\n",
        "\n",
        "translation, attention = translate_sentence(src, model, device, logging=True)\n",
        "\n",
        "print(\"모델 출력 결과:\", \" \".join(translation))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소스 문장: 도쿄의 니케이 지수는 9291.03으로 0.33 퍼센트 하락했다.\n",
            "타겟 문장: Tokyo's Nikkei is off 0.33 percent to 9291.03.\n",
            "전체 소스 토큰: ['<sos>', '도쿄', '의', '니', '케이', '지수', '는', '929103', '으로', '033', '퍼센트', '하락', '했다', '<eos>']\n",
            "소스 문장 인덱스: [2, 20084, 699, 35137, 37535, 15890, 12855, 0, 20155, 0, 15873, 34033, 11187, 3]\n",
            "모델 출력 결과: tokyo has also plummeted with the worlds largest economy <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2360KqGAcHNX"
      },
      "source": [
        "* 어텐션 맵(Attention Map) 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEhp1bcE33uU"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def display_attention(sentence, translation, attention, n_heads=8, n_rows=4, n_cols=2):\n",
        "\n",
        "    assert n_rows * n_cols == n_heads\n",
        "\n",
        "    plt.rc('font', family='NanumBarunGothic') # 폰트 설정\n",
        "    fig = plt.figure(figsize=(15, 25)) # 출력할 그림 크기 조절\n",
        "\n",
        "    for i in range(n_heads):\n",
        "        ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
        "\n",
        "        # 어텐션(Attention) 스코어 확률 값을 이용해 그리기\n",
        "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
        "\n",
        "        cax = ax.matshow(_attention, cmap='bone')\n",
        "\n",
        "        ax.tick_params(labelsize=12)\n",
        "        ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in sentence] + ['<eos>'], rotation=45)\n",
        "        ax.set_yticklabels([''] + translation)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzlH8RZEcIJF",
        "outputId": "b781cc43-ffd9-4652-ceee-eb12eb39672c"
      },
      "source": [
        "src = \"남한과 미국은 동맹적인 관계를 유지하고 있다.\"\n",
        "\n",
        "print(f'소스 문장: {src}')\n",
        "\n",
        "translation, attention = translate_sentence(src, model, device, logging=True)\n",
        "\n",
        "print(\"모델 출력 결과:\", \" \".join(translation))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소스 문장: 남한과 미국은 동맹적인 관계를 유지하고 있다.\n",
            "전체 소스 토큰: ['<sos>', '남한', '과', '미국', '은', '동맹', '적', '인', '관계', '를', '유지', '하고', '있다', '<eos>']\n",
            "소스 문장 인덱스: [2, 167, 31893, 5178, 11547, 6301, 26000, 36051, 39381, 684, 40381, 27333, 34346, 3]\n",
            "모델 출력 결과: the korea times reports the us and the us <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dnwiCsNlcKkU",
        "outputId": "19824a08-9265-453e-9cb1-ad52e7e99915"
      },
      "source": [
        "src = clean_string(src)\n",
        "korean_tokens = [line[0] for line in okt.pos(src, norm=True)] # 한글 형태소 분석 결과 추출\n",
        "\n",
        "display_attention(korean_tokens, translation, attention)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45224 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54620 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44284 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48120 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44397 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51008 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46041 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47609 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51201 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51064 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44288 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44228 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47484 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50976 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51648 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54616 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51080 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45224 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54620 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44284 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48120 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44397 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51008 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46041 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47609 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51201 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51064 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44288 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44228 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47484 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50976 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51648 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54616 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51080 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1800 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdEp459kf2Bz"
      },
      "source": [
        "#### <b>BLEU Score 계산</b>\n",
        "\n",
        "* 학습된 트랜스포머(Transformer) 모델의 BLEU 스코어 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76pGgi0IcS6M"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def show_bleu(model, device, max_len=80):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    index = 0\n",
        "\n",
        "    for i in range(len(korean_lines_test)):\n",
        "        src = korean_lines_test[i]\n",
        "        trg = english_lines_test[i]\n",
        "\n",
        "        trg = clean_string(trg)\n",
        "        trg = trg.split(' ')\n",
        "\n",
        "        pred_trg, _ = translate_sentence(src, model, device, max_len, logging=False)\n",
        "\n",
        "        # 마지막 <eos> 토큰 제거\n",
        "        pred_trg = pred_trg[:-1]\n",
        "\n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "\n",
        "        index += 1\n",
        "        if (index + 1) % 100 == 0:\n",
        "            print(f\"[{index + 1}/{len(korean_lines_test)}]\")\n",
        "            print(f\"예측: {pred_trg}\")\n",
        "            print(f\"정답: {trg}\")\n",
        "\n",
        "    bleu = bleu_score(pred_trgs, trgs, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
        "    print(f'Total BLEU Score = {bleu*100:.2f}')\n",
        "\n",
        "    individual_bleu1_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1, 0, 0, 0])\n",
        "    individual_bleu2_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[0, 1, 0, 0])\n",
        "    individual_bleu3_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[0, 0, 1, 0])\n",
        "    individual_bleu4_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[0, 0, 0, 1])\n",
        "\n",
        "    print(f'Individual BLEU1 score = {individual_bleu1_score*100:.2f}') \n",
        "    print(f'Individual BLEU2 score = {individual_bleu2_score*100:.2f}') \n",
        "    print(f'Individual BLEU3 score = {individual_bleu3_score*100:.2f}') \n",
        "    print(f'Individual BLEU4 score = {individual_bleu4_score*100:.2f}') \n",
        "\n",
        "    cumulative_bleu1_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1, 0, 0, 0])\n",
        "    cumulative_bleu2_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1/2, 1/2, 0, 0])\n",
        "    cumulative_bleu3_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1/3, 1/3, 1/3, 0])\n",
        "    cumulative_bleu4_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1/4, 1/4, 1/4, 1/4])\n",
        "\n",
        "    print(f'Cumulative BLEU1 score = {cumulative_bleu1_score*100:.2f}') \n",
        "    print(f'Cumulative BLEU2 score = {cumulative_bleu2_score*100:.2f}') \n",
        "    print(f'Cumulative BLEU3 score = {cumulative_bleu3_score*100:.2f}') \n",
        "    print(f'Cumulative BLEU4 score = {cumulative_bleu4_score*100:.2f}') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nuh4aVgCf3MU",
        "outputId": "02e6e302-dfeb-480f-d7d2-7a49b0b5b3eb"
      },
      "source": [
        "show_bleu(model, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100/2000]\n",
            "예측: ['the', 'international', 'atomic', 'energy', 'agency', 'said', 'that', 'the', 'international', 'community', 'was', 'the', 'first', 'time', 'of', 'the', 'international', 'community', 'in', 'the', 'international', 'community', 'to', 'be', 'allowed', 'to', 'be', 'freed', 'by', 'the', 'international', 'atomic', 'bombs']\n",
            "정답: ['the', 'skull', 'nicknamed', 'toumai', 'found', 'in', 'northern', 'chad', 'by', 'an', 'international', 'team', 'is', 'not', 'part', 'of', 'the', 'human', 'family', 'tree']\n",
            "[200/2000]\n",
            "예측: ['the', 'body', 'was', 'found', 'dead', 'and', 'her', 'mother', 'were', 'found', 'dead', 'in', 'the', 'southern', 'city', 'of', 'anonymity', 'because', 'of', 'the', 'death', 'toll', 'was', 'not', 'been', 'reported']\n",
            "정답: ['johnsons', 'eviscerated', 'body', 'which', 'police', 'said', 'they', 'found', 'after', 'receiving', 'calls', 'about', 'a', 'foul', 'odor', 'coming', 'from', 'the', 'apartment', 'was', 'in', 'a', 'state', 'of', 'moderate', 'decomposition', 'and', 'she', 'had', 'been', 'dead', 'about', 'two', 'days', 'medical', 'examiner', 'karl', 'williams', 'said']\n",
            "[300/2000]\n",
            "예측: ['the', 'number', 'of', 'people', 'are', 'expected', 'to', 'be', 'able', 'to', 'get', 'out', 'of', 'the', 'number', 'of', 'people', 'according', 'to', 'the', 'study', 'published', 'in', 'the', 'journal', 'of', 'the', 'journal', 'science']\n",
            "정답: ['studies', 'show', 'productivity', 'drops', 'when', 'temperatures', 'dip', 'below', '72', 'degrees']\n",
            "[400/2000]\n",
            "예측: ['he', 'said', 'he', 'had', 'been', 'charged', 'with', 'negligence', 'by', 'a', 'group', 'of', 'drug', 'trafficking', 'and', 'had', 'been', 'arrested']\n",
            "정답: ['the', 'couple', 'who', 'face', 'charges', 'of', 'bodily', 'harm', 'making', 'threats', 'and', 'coercion', 'were', 'released', 'on', 'bail', 'thursday', 'he', 'said']\n",
            "[500/2000]\n",
            "예측: ['the', 'israeli', 'military', 'has', 'been', 'killed', 'in', 'the', 'clashes', 'between', 'israel', 'and', 'the', 'militants']\n",
            "정답: ['israel', 'on', 'wednesday', 'also', 'released', 'the', 'remains', 'of', '199', 'fighters', 'from', 'lebanon']\n",
            "[600/2000]\n",
            "예측: ['the', 'us', 'geological', 'survey', 'reported', 'monday', 'that', 'the', 'us', 'geological', 'survey', 'was', 'carrying', 'off', 'from', 'the', 'coast', 'of', 'the', 'coast', 'and', 'pacific', 'coast']\n",
            "정답: ['a', 'strong', 'earthquake', 'struck', 'monday', 'near', 'some', 'greek', 'islands', 'close', 'to', 'the', 'turkish', 'coast', 'according', 'to', 'the', 'us', 'geological', 'service']\n",
            "[700/2000]\n",
            "예측: ['the', 'move', 'came', 'after', 'the', 'last', 'week', 'failed', 'to', 'sell', 'the', 'money', 'to', 'the', 'bank', 'in', 'june']\n",
            "정답: ['inbevs', 'original', 'offer', 'of', '46', 'billion', 'made', 'on', 'june', '11th', 'was', 'rejected', 'as', 'too', 'low']\n",
            "[800/2000]\n",
            "예측: ['but', 'the', 'sun', 'had', 'been', 'able', 'to', 'make', 'up', 'a', 'little', 'bit', 'of', 'a', 'little', 'bit', 'of', 'a', 'tree', 'in', 'the', 'sky']\n",
            "정답: ['but', 'she', 'points', 'out', 'that', 'the', 'jewelry', 'is', 'almost', 'always', 'made', 'from', 'cheap', 'metal', 'that', 'will', 'turn', 'yellow', 'or', 'lose', 'its', 'sheen', 'within', 'weeks']\n",
            "[900/2000]\n",
            "예측: ['iran', 'has', 'been', 'a', 'nuclear', 'test', 'for', 'the', 'first', 'time', 'since', 'tehran', 'iran', 'has', 'been', 'reported', 'in', 'a', 'statement', 'on', 'the', 'nuclear', 'bomb']\n",
            "정답: ['iranian', 'media', 'reported', 'that', 'tehran', 'fired', 'a', 'series', 'of', 'missiles', 'on', 'thursday', 'in', 'a', 'second', 'day', 'of', 'longrange', 'missile', 'testing']\n",
            "[1000/2000]\n",
            "예측: ['police', 'said', 'they', 'had', 'been', 'killed', 'and', 'two', 'men', 'on', 'the', 'bus', 'and', 'killed', 'at', 'a', 'police', 'station', 'in', 'a', 'neighborhood', 'where', 'they', 'were', 'killed', 'when', 'they', 'were', 'killed', 'at', 'least', 'two', 'people', 'on', 'saturday', 'police', 'said']\n",
            "정답: ['when', 'police', 'surrounded', 'the', 'men¡¯s', 'apartment', 'tuesday', 'they', 'found', '15', 'men', 'and', 'women', 'wielding', 'knives', 'and', 'shouting', '¡°sacrifice', 'for', 'allah¡±', 'a', 'police', 'spokesman', 'told', 'xinhua']\n",
            "[1100/2000]\n",
            "예측: ['north', 'korea', 'has', 'refused', 'to', 'comment', 'on', 'the', 'financial', 'row', 'over', 'the', 'norths', 'alleged', 'counterfeiting', 'of', 'nuclear', 'programs']\n",
            "정답: ['north', 'korea', 'declared', 'details', 'of', 'its', 'nuclear', 'program', 'last', 'month']\n",
            "[1200/2000]\n",
            "예측: ['president', 'bush', 'said', 'he', 'was', 'pleased', 'with', 'the', 'us', 'military', 'in', 'the', 'united', 'states']\n",
            "정답: ['president', 'bush', 'gave', 'a', 'positive', 'but', 'cautious', 'assessment', 'of', 'russia¡¯s', 'new', 'president', 'dmitry', 'medvedev']\n",
            "[1300/2000]\n",
            "예측: ['he', 'also', 'said', 'the', 'possibility', 'of', 'a', 'large', 'number', 'of', 'animals', 'to', 'find', 'out', 'of', 'animals']\n",
            "정답: ['von', 'hagens', 'says', 'he', 'relies', 'on', 'donors', 'not', 'only', 'as', 'a', 'source', 'of', 'specimens', 'but', 'also', 'as', 'representations', 'of', 'body', 'worlds', 'philosophy']\n",
            "[1400/2000]\n",
            "예측: ['ma', 'has', 'been', 'a', 'nationalist', 'party', 'in', 'china', 'since', 'independence', 'in', 'taiwan']\n",
            "정답: ['taiwans', 'new', 'president', 'ma', 'yingjeou', 'has', 'rejected', 'the', 'push', 'for', 'independence']\n",
            "[1500/2000]\n",
            "예측: ['the', 'new', 'york', 'times', 'reports', 'the', 'internet', 'is', 'being', 'paid', 'for', 'the', 'internet', 'to', 'make', 'a', 'better', 'life', 'in', 'new', 'york']\n",
            "정답: ['walter', 'scott', '24', 'put', 'his', 'soul', 'up', 'for', 'sale', 'on', 'new', 'zealand', 'internet', 'auction', 'site', 'trademe', 'and', 'so', 'far', 'has', 'received', 'more', 'than', '100', 'expressions', 'of', 'interest']\n",
            "[1600/2000]\n",
            "예측: ['north', 'korea', 'has', 'urged', 'the', 'north', 'to', 'return', 'to', 'the', 'north', 'korean', 'war', 'with', 'the', 'un', 'security', 'council', 'on', 'thursday']\n",
            "정답: ['but', 'the', 'governing', 'uri', 'party', 'still', 'underlined', 'the', 'importance', 'of', 'interkorean', 'economic', 'projects']\n",
            "[1700/2000]\n",
            "예측: ['the', 'official', 'said', 'the', 'blast', 'was', 'killed', 'by', 'a', 'helicopter', 'was', 'killed', 'by', 'a', 'helicopter', 'in', 'the', 'southern', 'town', 'of', 'kandahar']\n",
            "정답: ['regional', 'officials', 'said', 'he', 'died', 'in', 'a', 'shootout', 'when', 'marines', 'swooped', 'on', 'his', 'hideout', 'in', 'the', 'island', 'of', 'tawitawi']\n",
            "[1800/2000]\n",
            "예측: ['the', 'un', 'secretarygeneral', 'has', 'been', 'announced', 'by', 'the', 'un', 'envoy', 'to', 'the', 'un']\n",
            "정답: ['the', 'move', 'was', 'opposed', 'by', 'the', 'party', 'of', 'president', 'nicolas', 'sarkozy', 'who', 'has', 'been', 'trying', 'to', 'ease', 'tense', 'ties', 'with', 'beijing']\n",
            "[1900/2000]\n",
            "예측: ['the', 'dow', 'is', 'down', '3', 'points']\n",
            "정답: ['right', 'now', 'the', 'dow', 'is', 'up', '14', 'points']\n",
            "[2000/2000]\n",
            "예측: ['tokyo', 'japan', 'cnn', 'a', 'japanese', 'soldier', 'died', 'at', 'a', 'train', 'station']\n",
            "정답: ['japans', 'derailed', 'commuter', 'train', 'accident', 'has', 'killed', 'at', 'least', '69', 'people']\n",
            "Total BLEU Score = 2.03\n",
            "Individual BLEU1 score = 20.80\n",
            "Individual BLEU2 score = 3.58\n",
            "Individual BLEU3 score = 0.87\n",
            "Individual BLEU4 score = 0.26\n",
            "Cumulative BLEU1 score = 20.80\n",
            "Cumulative BLEU2 score = 8.63\n",
            "Cumulative BLEU3 score = 4.01\n",
            "Cumulative BLEU4 score = 2.03\n"
          ]
        }
      ]
    }
  ]
}